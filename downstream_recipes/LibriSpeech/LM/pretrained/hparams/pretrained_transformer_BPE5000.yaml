# ############################################################################
# Model: LM for LibriSpeech
# Tokens: Pre-trained LibriSpeech tokens
# losses: NLL
# Training: Timers and Such
# Authors:  Loren Lugosch 2020
# ############################################################################

# WORK IN PROGRESS. DO NOT USE THIS PRETRAINED MODEL UNTIL WE TRAINED
# THE LM WITH TRANSDUCERS
save_folder: pretrained_models
lm_ckpt_file: https://www.dropbox.com/s/x5czm16p66c1i2g/lm_model.ckpt?dl=1 # update
tokenizer_file: https://www.dropbox.com/s/5l8xecri9o2fk6l/5000_unigram.model?dl=1 #update
device: 'cuda:0'

# Model params
d_model: 768
num_asr_tokens: 5000


model: !new:speechbrain.lobes.models.transformer.TransformerLM.TransformerLM # yamllint disable-line rule:line-length
    vocab: !ref <num_asr_tokens>
    d_model: !ref <d_model>
    nhead: 12
    num_encoder_layers: 12
    num_decoder_layers: 0
    d_ffn: 3072
    dropout: 0.0
    activation: !name:torch.nn.GELU
    normalize_before: False

tokenizer: !new:recipes.LibriSpeech.Tokenizer.inference.pretrained.tokenizer
    tokenizer_file: !ref <tokenizer_file>
    save_folder: !ref <save_folder>
